# We made monthly AR from monthly PR for districts, with draws, or just the median.
# This script combines the monthly data into yearly data.
# It also adds values for reporting.
library(data.table)
library(sf)

## Combine individual files
data_dir <- "/ihme/malaria_modeling"
project_dir <- fs::path(data_dir, "projects/uganda2020/outputs")
proj_in_dir <- fs::path(data_dir, "projects/uganda2020/inputs")

# The canonical ID numbers come from the subcounties map, not the districts map.
sc_map <- "/ihme/malaria_modeling/projects/uganda2020/outputs/uganda_subcounties_2019_topology_fix/uganda_subcounties_2019_topology_fix.shp"
sc_dt <- as.data.table(sf::st_set_geometry(sf::st_read(sc_map), NULL))
dist_dict <- unique(sc_dt[, .(District)])
dist_dict <- dist_dict[, .(name = District, id = .I)]

# Inputs from the draws_dir, generated by gen_scaled_ar.R.
shp_path <- fs::path(proj_in_dir, "uganda_districts_2019-wgs84/uganda_districts_2019-wgs84.shp")
draws_dir <- fs::path(project_dir, "adam_median_draws")
# All results of this script go into the out_dir.
out_dir <- fs::path(project_dir, "adam_median_summary")

# Get the district names for the summary from the canonical source, this random shapefile.
# There is another canonical source, the subcounties file.
ug_dt <- as.data.table(sf::st_set_geometry(sf::st_read(shp_path), NULL))
# Use unique because some districts have more than one polygon in the shapefile.
# dist_dict <- unique(ug_dt[, "DName2019"])
# dist_dict <- dist_dict[, .(name = DName2019, id = .I)]

file_list <- list.files(draws_dir)

dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

# dt <- rbindlist(lapply(file_list, function(file) {
#   split <- strsplit(gsub(".csv", "", file), "_")[[1]]
#   dist <- as.integer(split[1])
#   draw <- as.integer(split[2])
#   data.table(dist, draw)
# }))
#
# dplyr::setdiff(dist_draw_table, dt)

# Read in every single file at once with all data.
dt <- rbindlist(lapply(file_list, function(filename) {
  dt <- fread(fs::path(draws_dir, filename))
}))

# This adds a NAME column where the name is the Dname2019 for the district.
dt <- merge(dt, dist_dict, by.x = "dist_id", by.y = "id", sort = FALSE)

all_output_in_one_file <- fs::path(out_dir, "adjusted_district_draws.csv")
write.csv(dt, all_output_in_one_file, row.names = FALSE)

## Generate annual PR summary by district and by year
dt[, y := as.integer(substring(date, 1, 4))]
sum_dt <- dt[, .(pr_median = median(pr), pr_lower = quantile(pr, 0.025), pr_upper = quantile(pr, 0.975)), by = .(name, y)]
write.csv(sum_dt, fs::path(out_dir, "annual_adjusted_district_pr_summary.csv"), row.names = FALSE)

## Generate bites per year by district and by year
bites_dt <- dt[, .(ar_sum = sum(ar)), by = .(name, y)]
write.csv(bites_dt, fs::path(out_dir, "annual_bites_per_year.csv"), row.names = FALSE)

## Generate annual AR summary by district
hold_dt <- copy(dt)
hold_dt[, c("date", "pr") := NULL]
hold_dt[, id := seq_len(.N), by = .(y, draw, dist_id)]
cast_dt <- dcast(hold_dt, y + draw + dist_id ~ id, value.var = "ar")
mat <- 1 - as.matrix(cast_dt[, 4:ncol(cast_dt)])
# This uses products of probabilities to get a correct yearly attack rate.
cast_dt[, annual_ar := 1 - apply(t(mat), 2, prod, na.rm = T)]
sum_ar <- cast_dt[, .(ar_median = median(annual_ar), ar_lower = quantile(annual_ar, 0.025), ar_upper = quantile(annual_ar, 0.975)), by = .(dist_id, y)]
write.csv(sum_ar, fs::path(out_dir, "annual_adjusted_district_ar_summary.csv"), row.names = F)
